http://euccas.github.io/blog/20170827/cpu-profiling-tools-on-linux.html

prof
------------------------------------------------------------------------------
AAargh.
So I captured a trace with Linux Perf. I rebuilt with -g so I could have symbols in the trace.
'perf report' seems to be able to see the symbols, but its UI is ... poor.
So I tried hotspot https://github.com/KDAB/hotspot#on-debianubuntu, which has a nice UI,
but can't seem to find my Elf file. I tried it via AppImage, so I'm wondering if the problem
is that AppImage can't see out to find my perf.data file.
So I tried strace on hotspot, but it can't see past some boundary used by AppImage; probably a
chroot-y thing.
Fine, I'll build hotspot from source and use that.
Hmm, it needs Qt>=5.10, and my ubuntu has 5.9.5.
No problem, I'll download the Qt installer. But wait, it requires you to LOG INTO A Qt ACCOUNT.
What the heck? Open $ource? Ugh. I give up. Gonna go try to find another profiler.

Okay. Turns out hotspot is getting symbols just fine; we're getting lost because we're not seeing
the libc stuff. How about getting at them?

apt install libc6-dbg
https://github.com/KDAB/hotspot/issues/128
... and we also didn't have symbols in YcsbMain.cpp, I think, due to Makefile.

rm -rf /tmp/data ; mkdir /tmp/data; perf record -g build/VeribetrfsYcsb ycsb/workload-profile.spec /tmp/data/ --veribetrkv
~/appimage/hotspot-v1.2.0-x86_64.AppImage --debugPaths /usr/lib/debug

http://www.brendangregg.com/perf.html
rm -rf /tmp/data ; mkdir /tmp/data; perf record -g --call-graph dwarf build/VeribetrfsYcsb ycsb/workload-profile.spec /tmp/data/ --veribetrkv

YYYAAAY! --call-graph dwarf was the answer!

rm -rf /tmp/data ; mkdir /tmp/data; build/VeribetrfsYcsb ycsb/workload-profile.spec /tmp/data/ --veribetrkv
After load phase,
perf record -g --call-graph dwarf -p 14148 sleep 10

Bottom-up flame:
====================
1.86% Bucket refcounts
2.8% refcount
4.8 + 1 +1+2 = 8.8% release_shared, ~shared_ptr, ::shared_ptr
1.55% shared pointer deletion?

3.3+13.8+2.5+1.4 = 21% in malloc, free
2% PartialFlush
6.5% malloc_consolidate

Top-down flame:
====================
16% ?? -- aargh symbols
7% Bucket assignment
5% deleting SuccResult calls Realloc!?
5% deleting KVLists
2.7% flush
4% top-level free !?
12.6% top-level malloc !?
4% rehashing something

Sure wish I understood why ??
Yeah. YcsbMain had no symbols. Change makefile to build .os and then link into executables!

gprof
------------------------------------------------------------------------------
Need -pg on link line as well as all compile lines, I guess.
gmon.out isn't emitted until exit() is called gracefully.
Ugh, 'gprof' barfs a pile of ugly text output and that's it. The only gui is
gprof2dot. Shudder.

callgrind/kcachegrind
------------------------------------------------------------------------------
sudo apt install valgrind kcachegrind
valgrind --tool=callgrind build/VeribetrfsYcsb ycsb/workload-profile.spec /tmp/data/ --veribetrkv
Oh yuck, it's not a sampling profiler, so it dramatically distorts runtime. :v(


==============================================================================
Travis' leak study
==============================================================================
./tools/run-veri-config-experiment.py workload=a device=disk ram=50gb config-8mb MaxCacheSizeUint64=512

> note that script only works on rob's machine on my user account
> because it hardcodes the mapping from device=disk to paths that only make sense there
> you should also probably note that the script builds Bundle.cpp, modifies it, and then builds the exe

> i also tried removing the subsequence-sharing optimizingin DafnyRuntime.h
> this helped, but mem usage was still at 6gb or so
> you can check out the branch ' dont-copy-for-subseq'
> if you want to see that
> also the branch is named wrong
> it should be copy-for-subseq
> because it does more copies

sudo apt install cgroup-tools
tools/create-cgroups.sh
tools/setup-clear-os-page-cache-binary.sh

./tools/run-veri-config-experiment.py workload=a device=disk ram=4gb config-8mb MaxCacheSizeUint64=512
./tools/run-veri-config-experiment.py workload=a device=disk ram=6gb config-8mb MaxCacheSizeUint64=512
  -- eventually killed by ulimit (cgroups?) After swapping laptop to pieces. 14m real 5m user 3m sys

./tools/run-veri-config-experiment.py workload=a device=disk ram=3gb config-8mb MaxCacheSizeUint64=256
Added heaptrack.

Proposed experiment: at op #3,000,000:
- count up how much stuff Dafny thinks vbfs has allocated.
- count up how much /proc/stat thinks vbfs has allocated
- count up how much malloc thinks vbfs has allocated
Then sync/evictEverything, and produce the same report.

rm -rf /tmp/data; mkdir /tmp/data
bash
ulimit -m 4000000
build/VeribetrfsYcsb ycsb/workload-profile.spec /tmp/data/ --veribetrkv > exp/x-20m

make build/VeribetrfsYcsb

After 17m, we blew through ulimit m=4000000
OS says we have used 3.27e9 bytes,
malloc counts 2.71e9 bytes -- maintaining the 20% probably-fragmentation
overhead. Willing to believe that.

At the point of the crash, 88% of the memory is allocated as 150-byte
seq<unsigned char> (15,989,114 allocations).
The Node count has grown to 350, maintaining a ratio of ~45k records per node.
(That's right for a 150-byte value plus about 25 more bytes -- the key?)

The next 6% (176MB) are 1198 seq<Message> arrays, these must be the containers
holding all the inserts. That's 11 bytes per allocated value; plausible.

The next 5% (141MB) are 1198 seq<unsigned char>, 9 bytes per allocated value. I
guess these are the packed byte arrays that the messages point into.

And now we've accounted for 99% of the allocated memory. Doesn't smell leaky.
So the gaps are in two places:

1. There's a 20% gap between what we've received from malloc and what we get
billed by the OS in the /proc heap. I'm guessing malloc fragmentation. It's
conceivable that we could choose a tuned allocator. We could also just discount
our cache constants by 20% (use 425 blocks instead of 512) for now.

2. There's a 20% (727MB) gap between what we see in the /proc/maps [heap]
and the point where the ulimit of 4e9 fired. I don't have a hypothesis about
this.

Next ideas:
- drop ulimit to 1GB, to make the experiment 4m long
- cat /proc/maps for easier postmortem.


rm -rf /tmp/data; mkdir /tmp/data
bash
ulimit -m 1000000
time build/VeribetrfsYcsb ycsb/workload-profile.spec /tmp/data/ --veribetrkv > exp/x-20m-u1gb
Well that's pretty freakin' weird. it happily blew past 1.2g rss in top.
I'll just run the original experiment again.

next -m4000000 try died at 2963599360, which is a 34% overhead.

heap was 2938056704, every page in /proc/maps was 2962866176,
a 23MB (a 1% offset). So THAT ain't it.

Let's try ulimit -v?
2962866176

Okay, -v hit pretty hard at 970825728.
And with 2gb, at 1975689216. (1999806464 with 23MB overhead!)


Next experiment: run MaxCacheSize down to 400 (by manually editing Bundle.cpp).
Run again with -v 4000000, and expect the proc size to stay below 3.75gb
(400/512*4*1.2).

9.5 minutes got through 4.8M ops, so this is at least a 40-minute experiment.
(Maybe longer since the tree is growing a bit.)

And so it crashed in 15 minutes. 

Travis saw stabilization on an intended-4GB config at around 7GB,
and we want to see some cache stabilization. So we decided to set
MaxCacheSizeUint64 = 200 and run to stable memory consumption.
exp/x-20m-uv4gb-200count

Okay, it was consistently dying at 15m30s *real* time. Because that's about
1000s; I was running out of the 1024-file-handle ulimit. Because my profiler
was opening /proc/self/maps and not closing it.

Summary of what we learned.
The problem we set out to solve is that our actual memory consumption would
greatly exceed our predicted size (e.g. 8MB * MaxCacheSize), causing the
program to swap, creating orders of magnitude penalty on query performance.

[malloc-exp/heap-to-malloc.png]
We discovered that, during the growth phase (0-400s in the figure), we see
a pretty consistent 1.2x overhead on malloc. The interesting observation
is that, once the we hit cache pressure and start evicting blocks, the
malloc behavior changes dramatically, converging on a 2.34x overhead.
Apparently our memory freeing patterns change once we start evicting.

The good news is that there does seem to be a kinda-stable constant there.
So the proposed actions:
* We can solve the swapping problem by discounting the memory by around 2.4x.
* It may be worth looking at the allocation-size distribution. If it's super
modal, maybe we can tune the allocator.
* It may be worth understanding whether rocks is paying a similar price.
Perhaps Jon's malloc investigator can be made to work there.


Okay, we've learned something!
I used the mmap pool to separate big (>1MB) allocations from little.
The problem was that the heap that grew during the warmup phase never
got cleaned up, and stayed sprayed out, so it hurt RSSz as well as VMSz.

So the next experiment was to try evicting everything every so often (100s),
thinking that maybe we can zero out that heap before it gets too big.
[malloc-exp/evictions-truncated-heapmallocplot.png]
And viola! the heap basically stops growing at the first evict-everything!
The overall memory usage is about 1.9GB, which is about 1.6GB
(expected, 200 blocks * 8MB) times the 20% malloc overhead. Liveable!

Because mmap-pool is enabled, the paged-in blocks show up on the graph (in
orange) -- so you can see that as we recover from the first eviction, the
malloc line jumps right back up to its trendline, but now it's backed by a
vertical blast of orange block page-ins (of the top of the tree) replacing the
original red stuff.

I'm trying the same experiment now with mmap pool disabled to see if it's
actually necessary.

I should also try varying the period between the evictions.

Now this definitely slowed things down overall, because the inflection
point doesn't happen until about 570s, which is about 100s later.

Hmm, maybe there's a less-destructive way to accomplish this.
We could mark blocks by how they were built, and if they're built out of
little allocations, replace them with a new block (marshall/unmarshall?)
with equivalent semantics but with a clean allocation representation.
The idea would be that eventually all the startup-phase cruft would get
cleaned out, reopening lots of contiguous memory.

So here's a cute little experiment: First evcition at 25s, then 50s intervals
thereafter.
[malloc-exp/evictions-nommap-25s50s-heapmallocplot.png]
We made room for 25s worth of inserts with the eviction, reused that memory
during [25-50s], then had to grow the heap more during [50-75s].

This suggests that one strategy would be to hold the first eviction until
the inflection point -- that is, do a full eviction at the first point we
have to evict a single block. That'll reset the heap from the small-objects
phase. After that, if the workload isn't phasic, we may not even need to
do anything else!

Okay, so I tried to simulate that strategy with another experiment.  Here, I
did the first eviction at 450s, then 200s intervals.  Look at 450s on the
graph: we empty most of memory (blue line drops near zero), but when we page
the blocks back in (blue line climbs), the orange line (os-map-total) climbs,
too, about the same amount -- apparently even though we emptied most of memory,
the allocator still had to go buy fresh pages to host big 8MB blocks.

That indicates that what was left in memory was enough and well-enough
fragmented that it managed to keep all the holes small.  So the
periodic-eviction works in part because it's recycling those small holes, using
them for more small allocation demands.

Note that the 1.5GB gap between blue and orange is essentially permanently
lost, just as it was when we didn't do the eviction at all. And so the overall
memory demands converge to >3.4GB: the 1.9GB we need in the previous successful
run, plus the 1.5GB we "broke".

So here's a run with MaxCacheSize=400, evicting everything every 100s.
That's 3.2GB of live data; 3.2*1.25=4GB. ulimit -v 4000000.
[malloc-exp/evictions-nommap-100s100s-cache400blocks-heapmallocplot.png]
It successfully completes 20m operations, which I guess is a first for us?
--      throughput      duration(ns)    operations      ops/s
veribetrkv      throughput      2799068554254   20000000        7145.23
Even after 20m ops the memory profile hasn't yet converged, but it indeed
is below the forecast 4GB.


tools/run-veri-config-experiment.py workload=b device=disk ram=4gb config-8mb > malloc-exp/cgroups-veri
tools/run-veri-config-experiment.py workload=b device=disk ram=4gb rocks > malloc-exp/cgroups-rocks


Okay, some actual legit cgroups-enforced results, on workloadb (95% read):
  --	throughput	duration(ns)	operations	ops/s
  rocksdb	throughput	100483121524	5000000	49759.6
  veribetrkv	throughput	493813475855	5000000	10125.3
So in this example we're 5X behind.  That's a lot better than 100X behind,
though! And we have all of the following limitations:

- my abusive evict-everything-every-100s policy
- my malloc thingy interposing on every allocation (because that's the same
  branch where my eviction policy is)
- we're still not exploiting the full cache; after 5M ops, veri only got to
  1.7GB. So we're not getting as much benefit from the cache as rocks is.
  Maybe I should run for longer.

So we blew through memory. But looking at the malloc scopes, we have about
10M bytes allocated per Node, and that number stays pretty stable for the
short life of that experiment. 343*10M = 3.4GB, so we're not way off. In
fact, we predicted about a 25% cost at the fringe (although I forget the
reasoning), and 10M = 8M * 1.25. So maybe we just need to discount the Node
count a bit more.

[malloc-exp/wprofile-amass-discount]
Nodes=300, with tree->kvlist repacking
200 should have worked (using about 2GB), but it just blew right on past.
Bytes/node climbed enthusiastically once the cache filled. Trying again with 50.

[malloc-exp/wprofile-amass-discount-50]
Okay, well, that did stabilize, with 600MB in [byte], 0.9GB in large
allocations, and 1.2GB of OS. (Quite a lot of malloc still; now in the large
region?)
This suggests we can probably push up to at least 150 nodes.
Will that stabilize nicely?

[malloc-exp/wprofile-amass-discount-150]
Okay, so 150 blows out.
[malloc-exp/wprofile-amass-discount-100]
Okay, so 100 fits.
Things we know, from the runs that complete:
- before the cache fills, the byte[]:Node ratio is a plausible 8-8MB
- after the cache fills, the ratio climbs up to (14, 24) before settling in.
  Ouch!
- The byte[]s explain about 0.7-1.0 of the malloced space.
- At the end of discount-100, that ratio is 76%. The next 23% is
  explicit-seq<byte>. Where does that come from?
I think that's really the burning question.
-- a bunch are 512-byte allocations from ycsb
-- and then, after 2.4Mops, we see page-in events.
-- and then two 7k allocations from the parser

[malloc-exp/wprofile-amass-discount-100-timeseries.png]
Discovery: Amassing doesn't work, because the amassed objects seem to get
hung-onto by a thread. In the middle graph, note we have 100 Nodes in
this experiment. In steady state, once we start page-in at 250s, there are
also 100 faulted-in large (8M-ish) blocks. But then we have some 600
"amass"ed blocks: 6-ish per Node.
So, hypothesis: the data getting inserted gets amassed. The amassed blocks
get pushed down the tree, data spread out across the leaves. Leaves get
evicted. Some of the amassed data lands in a frontier Node, and the rest
gets evicted. But the frontier nodes are enough to keep that amassed allocation
around for a long time. The amassed data was a single bucket when it started,
destined for a single Node at the second level of the tree. But it's
spread across ~8 nodes at the third level, which is where the frontier of
a 50-node tree lands. If our tree got deeper, the amassed nodes would hang
on by even smaller threads, and we'd see a 64:1 ratio, I bet!

I'm not entirely sure why the pagein blocks don't have the same behavior.
Wouldn't they get pushed down the tree? I guess pageins for tree-top Nodes
are never stranded (because treetop nodes are evicted), and pageins for
evicted nodes are GCd, because they can only be referenced by Nodes below,
which in general are also evicted.


[malloc-exp/wprofile-amassparent-100-timeseries.png]
Next idea: amass on the way down the tree.
First variant because I'm lazy (and maybe it'll improve perf): only amass
the parent bucket. Idea is that children will eventually get freed; we fix
the problem where a parent hangs onto one stray allocation even after most
of the messages have been pushed below. Children are evicted, ancestors are
not. On the other hand, this may fail if *siblings* hang on to those apron
strings.
Yeah, we're already at 2X amass vs Node. Well, actually that would be
fine if they're smaller (1MB) allocations. Hmm. I guess I need
amass-bytes/Node? Overheads are staying pretty tight, which is good. Oh,
but they don't suck until page-in time anyway.
Didn't really work; overall allocations are still pretty high.

[malloc-exp/wprofile-amassflush-100-timeseries.png]
Okay, so let's amass both parent and children on flush. This may still
not work, since the root still has an 8MB allocation that hangs around until
flushed to each child -- but that's only a single Node.


Okay, one totally reasonable possibility:
With the current strategy, replacing a bucket at a time, the worst case
is that each node is paged-in (8MB); perhaps
the paged-in seq is never deallocated because of refs to keys as pivots.
Then each bucket gets reassembled with amass. This would explain a 2x
factor - it would explain amass not dropping off once page-in starts.
(I mean, it eventually does, but not apparenttly as an effect of page-in.)
But that's not the whole story.

Okay, so it would be NICE to keep track of all the amass allocations and
then check in later and look at the sptr ref counts. But I fought gdb for
a couple hours trying to look into the shared pointers. The _M_ptr technique
I used once doesn't seem to be in play; we're now using different
header (/usr/include/c++/v1/memory). I used clang++ -E to ensure I was
looking at the right guts, and a sptr should have a __cntrl_.__shared_count
field, but for some reason gdb can't see it.
The use_count() method is available, but if I'm going to do this at the
source level (instead of the debugger level), I'm gonna have to use
weak refs, too. Blearggh.


Next question is: are we really sure that the 'amass' allocation microscope
is finding the allocations we think it is? Andrea suggested walking through
the underlying sptrs and adding up all of their usage as viewed from inside
C++.


[malloc-exp/wprofile-amass-countpivots-timeseries.png]
Okay, we're amassing all the children on the way out of flush (yay), and we're
amassing in Split, and we're CopyKey-ing pivot keys.
We also added a special hack to label amass allocations very precisely.
Finally, we see most of what we want: the reachable-underlying count eventually converges
to amass live allocs, and the overall malloc is 0.8GB, what we'd hope.

Some mysteries survive:
(a) Why isn't live amass allocs exactly equal to underlying alloc count? (graph 6)
(b) What are we counting in graph 5? pkvs? I thought my counting code didn't touch those.
  ...maybe they're being accessed by way of pivot keys, now that we're counting those.
(c) OS/mmap ratio is still creeping up (graph 2). Maybe we should still use a big-pool policy?

And open questions:
(d) Do we really need to amass splits? flushes? Maybe the pivots were the ony
  thing causing the real trouble.

[malloc-exp/wprofile-amass-nosplit-timeseries.png]
Tried turning off amass during split, to see if it matters (open question d). Doesn't seem to.

[malloc-exp/wprofile-amass-countpkv-timeseries.png]
Chasing down the pkvs. (split still disabled, which reduces fidelity, but doesn't seem to be
a big factor.) Taught sampleNode to walk the pkvs, and now underlying_sum.bytes (graph 5)
correctly
split is back on (for high fidelity counting), and I'm now chasing down the pkvs
as well, to try to address mystery (a).
Conclusions: the underlying_sum.bytes curve (graph 5) now counts up the pkvs,
so it includes everything, and pretty much looks like the malloc_total. So the places in the
previous graph where underlying.bytes exceeded amass were likely places where the counter
managed to cross into pkv-space and count up those allocations. Weird that it didn't count all of
them, but I think we're going to not worry about that graph anymore.

Observations on mystery a: the 'amass live alloc count' (orange line) is definitely *only* stuff
allocated in AmassKvl (KVList.i).
One crazy possibility: the root pivots don't come from there. But that's like, 8 allocations.
We're undercounting 100.
Amass happens at tree_to_kvl, flush, and split.
And why, if we're not reaching 100 allocations, aren't there lots of bytes associated with the
"missing" count? Are there small amass-ments that are being selectively reachable by some
non-Node path!?
[malloc-exp/wprofile-amass-countpkv-head]
So here's the same file truncated to ~340s, where the two have diverged pretty well.
As we'd expect, a single node (the root) is type 'tree'; the rest are kvl.
Underlying counts 283 (674MB).
in_amass is 378 (669MB).

[malloc-exp/wprofile-amass-parent-pivot]
Only parent amass & pivot copy-key in place; child amass and split amass disabled.
Oh yeah, that's a disaster. Wish I had my ulimit on.

------------------------------------------------------------------------------
Okay, amass is in. Let's run some experiments.
tools/run-veri-config-experiment.py workload=b device=disk ram=4gb config-8mb > malloc-exp/cgroups-veri-wkb.data
-- happy as a clam because it doesn't insert much.
tools/run-veri-config-experiment.py workload=a device=disk ram=4gb config-8mb > malloc-exp/cgroups-veri-wka.data
... killed even with cache size 343 (malloc overhead 1.5)
time build/VeribetrfsYcsb ycsb/workloada-onefield.spec /tmp/data/ --veribetrkv > malloc-exp/manual-wka-100.data
100 blocks settles in at about 1.7GB, or 17MB per node. At 4GB, we can afford about 240 such nodes,
so MALLOC_OVERHEAD should be set to 2.2.
...that makes nodes=232, which crashes.
200 nodes seemed to squeak by (4.2GB after 3500), so let's try that. overhead=2.56. Erk. This sucks.
need to turn malloc stuff back on.

So what the heck. I guess I need to turn malloc back on so I can understand why the overhead is 2.56 now.

So the page fault was just malloc_accounting tripping on the null return from underlying malloc,
due to blowing through 4GB.

With 200 blocks. I see a 33% overhead in the malloc graphs: OS mapping is 4GB,
malloc total is 3GB. That's 15MB/node. What broke!?  CopyKey is enabled. Only
thing disabled was left/right splitting. Let's re-enable it.

[malloc-exp/manual-wka-200-amasssplit.data]

Okay, so amassing split was indeed relevant. We're still at an overhead factor of 2X.
A hypothesis: pkvs hanging around? But Amass should cure that.
Oh, no, amass on the parent only replaces one bucket.

[malloc-exp/manual-wka-200-amassspagein.data]
...does an amass on page-in to break the pkv into buckets, and also recopies the pivots, to shake
loose any connection to the underlying pagein pkv.

Hooray. I think I've finally gotten a handle on the memory situation.

Wednesday I thought I had everything tied up, and it was "just a matter of" measuring.
Then I ran the thing, and found that 200 8MB nodes were gobbling 3.2GB of malloc. That's 100% overhead.
I realized the problem: once we start paging in, we pull in a monolithic 8MB block, then as we flush changes
through it, we replace some of the buckets with new 1MB (isolated "amassed") allocations. I think that,
in steady state, the cached blocks are responsible for 8 1MB buckets, plus they hang onto the original
8MB PKV paged-in block via at least a pivot key. (Or maybe they have 7 buckets and one bucket referencing
the PKV allocation.)

So I added code to, upon page-in, copy out the keys, and re-copy (using the same bucket-amass method)
the buckets into fresh buckets. From the memory perspective, this seems to be mostly working.
The amassed memory adds up to about 2GB (perfect!). The total malloc is about the same.
The OS allocations, once page-in begins, show about a 500MB overhead. I'd guess this is a swiss-cheese
memory layout problem (since page-ins ask for 8MB allocations), except that now we should only ever have
about one 8MB unit in memory at a time. So why a 500MB tax?

But these results aren't really final:
- the total allocations continue to climb. Will they level off?
- it's hella-slow. Perf dropped to 1Kop/sec. I could imagine a 2x drop, since
  we're recopying all of the memory on every page-in. But perhaps my copies are
  slower than the memcpys in the natively-supported IO ops? So my experiment has only reached op #3000;
  it'll be at least an hour before it reaches op 7,000.

Takeaways:
- it would be nice to understand why the OS allocation jumps up so high after page-in time. But maybe
  unnecessary, depending on the ultimate fix.
- the ultimate fix is probably to store the nodes on disk as a set of 8 PKVs, so that we don't need to
  recopy them. And when we page them in, recopy the pivot keys. (Or are the pivots represented separately
  in the marshalled representation? The pivot recopy might have been a waste of time.)
- also: Reprs will turn me into a supervillian. Horrible.

Rob suggests that os-vs-malloc gap may be libasyncio stacks.
  - well, it ain't stacks. All of the mapped bytes are in the heap mapping.
Try disabling stupid sequence-copy optimization.
Try measuring memory usage from the inside (dafny view).
  -- how are we still seeing amass @ 2GB, expecting 1.6GB.
Graph read, write I/O rates.  Cache hit rate.

[malloc-exp/manual-wka-200-amassspagein-4.data-timeseries.png]
Looking from the inside!
- tree-node-count I'd have expected to be 1. It gets up to 12, 7 during the
  startup phase.
  = What's going on there!?
  - Nothing. Those are buckets. They're all in the root node. Data labeling corrected.

- Most of the GB are in message bytes (yay!). Keys seem to be about 24 bytes.
  = check in YCSB.
  - yep. Dropped into the debugger, and saw a key 'user181211450603064119781\002', which is 27 bytes long.

- There are only 3758 bytes in Pivot keys across 200 nodes? That's 19 bytes per node,
  or less than one key per node. That feels wrong!
  = count pivots, keys, and messages.
  - sure enough, At 158 Nodes, there are 123 pivot keys!
  - average buckets/node is below 2!
  - turns out that's expected, because all of the leaves have 1 bucket.

- The messages (95% of the data) get up to about 1GB at op2500; at the same
  time, malloc is already around 1.8GB! This is all before we even begin page-in!
  # How is that possible?
  # Next step: turn the underlying allocation counter back on; see who it agrees with.

[malloc-exp/manual-wka-200-noseqopt-1.data]
Re-enables dbg_underlying_len walk.
Disables subsequence optimization.
...was a disaster. Load took forever; was running at 343 ops/sec.

[malloc-exp/manual-wka-200-underlying-1.data]
Re-enables dbg_underlying_len walk.
Leaves subsequence optimization enabled.
  yeah, that's loading at 7.9 Kops/sec.
Okay, so yeah, underlying is seeing pretty much all the malloc mass.

So we *know* (due to underlying allocations; Framework.cpp/sampleNode) that
we're reaching all that extra memory, just in kvl & pkv keys & messages. That's
surprising, since we're amassing the crap out of everything. Job 1 is to get
underlying to match internal-bucket-message-bytes.

Next question: Do particular nodes or buckets have the overage disproportionately?
Would it be useful to print out, for each node, the disagreement between underlying
and internal, and then sort them by ... depth in the tree?

First thing: let's associate each underlying with the referrers and size-of-reference.

[malloc-exp/manual-wka-200-underlying-2.data]
Adds map from underlying reference to the set of nodes that mention it. Prints counts.
Okay, so the counting is working. I'd next want to know:
- I see some refs being pointed at by 16 unique noderefs! Why?
- More importantly, do the pointers at noderefs use all the data in the ref?
  - could use a conservative count, assuming nobody uses the same data twice, to avoid
  having to track intervalsets.

Okay, so we can, in fact, see the leaks from this vantage.
Sorting ref_used rows by how much was leaked, I find:
- some negative leakage: these are places where the sequence really is in play
  in two places at once, and my conservative count was wrong. There were seven such
  sequences. Probably in the write queue or something; not going to get too worked up.
- about half of the refs were precise (zero leak)
- the other half were leaky.
  The distribution of original allocations isn't obviously bimodal; there's a hop near 4MB.

Okay, so wtf. We know that there are exactly 1 tree nodes and exactly 0 pkv nodes, so these
all have got to be coming from kvl nodes. Don't we amass on every split & flush?

So now I've caught it in the debugger with 7 nodes, a grand total of 5 pivot keys
(root, and 5 to separate the six kids).
I think we need to label the buckets to understand which code path is generating each one.
I'd have thought every one of these children would have come from the root via flush.

BucketImpl ctor invocations:
- ctor -- mkfs
- pkv -- pagein the mkfs
- kvl -- amass (after page-in)
- ctor -- runFlushPolicy / grow
- ctor -- splitLeftRight
- ctor -- amass left
- ctor -- amass right
- InitWithWeight -- MutBucket::CloneA (SplitChildLeft)
  ** this looks pretty suspicious! NodeImpl.SplitChildLeft (and Right).
  -- except those don't split byte seqs.

Hard to find the subsequence leaks with the debugger, because the Amass code takes its
own subsequences of the array on purpose.

Hey, here's a neat trick for a gdb condition variable. I want to skip cases where
a particular frame is on the stack, two above the breakpoint:

cond 5 dbg_underlying_len - (hi - lo) > 50000 && hi-lo > 50000 && ((void***)$rbp)[0][1]!=0x40dffb
So, if we want to look N frames up for pc address 0x40dffb, use N+1 *'s and N-1 [0]'s.

Well, THAT'S interesting: the only code *generating* big subsequence references
is the Amass code!  So somehow those subsequences are escaping. Each Amass
should be glued to a Node; perhaps we can set a Node association during the
Amass step and then get angry if it ever parts?

So maybe I can track this down from the instrumentation: any two nodes talking
to the same underlying allocation represent an amassed allocation that was
supposed to stay with its original owner, but leaked.

Looks like the first PartialFlush happens at op num 59368 (in the current workload-profile config).
So I can run a trace before and after it to confirm my suspicions.

back at ~26k
grow

repivot
  - some leaf node gets cut in half. Is that ... the root? Nope, it's node 1: the first pkv-lookin' node.
split
  - parent=0 and child=0. Constructs a new pair of children, and replaces the single child ptr in the
  parent with two.
  - and SplitChildLeft does call MutBucket.CloneSeq, which is the only place where we take a subsequence,
  but it's a subseq of buckets, not of bytes. Arrgh.
  - MutableBucket.CloneSeq calls .Clone, which copies the kvl into place.
flush
  - pushes writes from the parent down to buckets in the children.

so which one is guilty?
I mean, split makes new nodes -- which should be clean,
and then flush flushes new buckets into those nodes -- 
ah. So say we Split, and then only flush onto one side. Would that do it?
Well, we can at least rule that out by adding an Amass in the Clone.

And that worked. We went from:
allocationreport refset ref 39192832 refd by 2 noderefs
allocationreport refset ref 70288656 refd by 3 noderefs
allocationreport refset ref 74408624 refd by 1 noderefs
allocationreport refset ref 23808784 refd by 1 noderefs
allocationreport refset ref 39184688 refd by 2 noderefs

To:
allocationreport ref_used ref 53160256 underlying 4112868 used_total 4112868 refd_by_nodes 1
allocationreport ref_used ref 39200944 underlying 4112868 used_total 4112868 refd_by_nodes 1
allocationreport ref_used ref 70296768 underlying 4119920 used_total 4119920 refd_by_nodes 1
allocationreport ref_used ref 49040288 underlying 4119920 used_total 8239840 refd_by_nodes 2
  -- this is a bunch of alternated keys/values.
allocationreport ref_used ref 82649600 underlying 4137044 used_total 4112934 refd_by_nodes 1
allocationreport ref_used ref 38787360 underlying 24110 used_total 24110 refd_by_nodes 1

Which kinda looks better.

So which nodes are taking those duplicate references?
Debugging slowly, it's nodes 4 & 2.

Yup, let's just print that stuff out:

allocationreport refset ref 39196976 refd by 1 noderefs
    1
allocationreport refset ref 70292800 refd by 1 noderefs
    1
allocationreport refset ref 20797824 refd by 1 noderefs
    0
allocationreport refset ref 49036320 refd by 2 noderefs
    2  4
allocationreport refset ref 53156288 refd by 1 noderefs
    3
allocationreport refset ref 61406288 refd by 1 noderefs
    4
allocationreport refset ref 39188832 refd by 2 noderefs
    0  1

Node 0 -> child 4
Node 0 -> child 3
So if we treat nodes 1 and 2 as dead nodes (need eviction), we're okay.
That's annoying. Well, maybe we've fixed the problem (run a bigger experiment
outside the debugger), or maybe we need to clean old crap out of the cache?
Need some way to account for that stuff. How long does it linger?

[malloc-exp/manual-wka-200-amassclone.data]
...full experiment with Clone amass turned on.
Nope. Still leaky.

Is this stuff Frozen?

Okay wait this is stupid. If we're amassing correctly, nothing should be
shared. 0 shouldn't be sharing with 1, and 2 shouldn't be sharing with 4.

Wait, somehow before the splitasaurus, we have 0->1, both *tree* nodes! How
is that even possible? Okay, Grow is sort of simple; it just make a new node
with no pivots, a single newly-minted emptyKvl bucket, and a single pointer to
the child, which is the same tree-node representation. No carried-over
pointers.

Okay, so the first crime is definitely being committed in the repivot-split-flush
sequence. Sure would be nice to call allocationreport on those suckers.
We're looking for sharing between 2,4 and 0,1 (24 bytes -- just a key!)

Pretty confident repivot is not it, since it replaces refs to a bucket with
new buckets in a replacement node. (The other node *might* be loitering in the
Frozen table, but that's okay.) And we Amass on top of that! Could confirm,
but I don't think it's here.

doSplit(0, 1, slot 0)
  splitDoChanges has left__childref=2, right__childref=3, parentref=0.
...and indeed there was a not-CopyKey'd pivot in splitDoChanges.
Patching it eliminated the 24-byte (single-key) shared between 0,1, **FIXED**
leaving only the 2,4 sharing.

So the guilty party must be Flush.
Flush is called on (0, 2), but the leak is 2,4. Where does 4 come from? It's newchildref.
Oh baby we're onto something hot now.

child.pivotTable -- carted over from the input child node. Oops; pivotTable needs to be copied. **FIXED**
child.children -- same as before and that's fine.
child.newbuckets -- well damn, we left the new child buckets amass in place but
  the assignment to the return variable commented out. Aaiieee. **FIXED**

Okay, yay! internal-bucket-message-bytes now agrees with underlying!
There are still ~25MB of seq-from-array stuff (Messages, seq<char>); ~20%. Not sure
how we lost track of that. But let's worry about that later.
And we ARE under 1.6GB total now!

Next thing to fix: lazily amass pkvs.

---
Okay, so veri-8mb was much slower than manual-wka-opt-350 (also 8mb).
That's weird.
- re-run both experiments; figure out what changed
- factor out the plotting tool so I can plot comparative graphs with rocks
- gather os-heap usage info from rocks, just because I'm curious.

[malloc-exp/veri-8mb-wka-3.data]
tools/run-veri-config-experiment.py workload=a device=disk ram=4gb config-8mb > malloc-exp/veri-8mb-wka-3.data
...was dribbling along at 320ops/sec. But manual-wka-opt-350 had finished out at
1360 ops/sec. I think all of the allocation reporting was actually hurting us;
maybe making a mess of the cache? Surprising, though. Trying again with
less allocation reporting.
Argh, the 8mb curve is still dropping well below manual-wka-opt-350. What gives!?

perf record -p XXX
~/appimage/hotspot-v1.2.0-x86_64.AppImage --debugPaths /usr/lib/debug


------------------------------------------------------------------------------
Next steps:
X check in what works now.
- merge back to master...?
- instrument rocks for:
  - I/O count, byte count
  - CPU utilization
  - cache hit rate (implied by I/O count, I guess)
Bogus behavior is evident after only 10M ops (45 minutes in veri).

------------------------------------------------------------------------------
tools/run-veri-config-experiment.py workload=a device=disk ram=4gb rocks > malloc-exp/cgroups-rocks

gdb --args ./build/RocksYcsb ycsb/workloada-onefield.spec /tmp/veribetrfs/ --rocks


tools/run-veri-config-experiment.py workload=a device=disk ram=4gb config-8mb > malloc-exp/veri-8mb-wka-ioperf-1.data

tools/run-veri-config-experiment.py workload=a device=disk ram=4gb rocks > malloc-exp/rocks-wka-ioperf-1.data
[malloc-exp/rocks-wka-perf-1.data]
Rocks/wka with ioaccounting and stat-accounting fields

Hypothesis: My NVMe SSD is good at small IOs.
It's a MZVLB1T0HALR, speced at 3200MB/s read, 380K IOPs (2.6usec seek).

My disk is usb-Seagate_BUP_Slim_BK_NA9Y4VC9-0:0, about 1TB.
Apparently it's a hybrid SSD+HHD, which I guess sucks for us.
https://www.storagereview.com/review/seagate-backup-plus-slim-review-1tb-2tb
says 120MB/s, 40IOPS for 2MB random transfers, 308IOPs for 4K.


[malloc-exp/rocks-wka-ioperf-disk.data]
tools/run-veri-config-experiment.py workload=a device=disk ram=4gb rocks > malloc-exp/rocks-wka-ioperf-disk.data

[malloc-exp/veri-8mb-wka-ioperf-disk.data]
tools/run-veri-config-experiment.py workload=a device=disk ram=4gb config-8mb > malloc-exp/veri-8mb-wka-ioperf-disk.data

With per-opn instead of per-time syncs:
  - initial tput down at 3.5K. That's weird! If we could retire 5K ops + a sync
    every second before, why not in this configuration? Hmm.
  - There's still a pretty severe dropoff at 6M ops (full cache).
I modified YcsbMain to account for sync time, and sure enough, many were
in the 1s neighborhood.

Let's get the syncs down to a manageable level. I set the period to 50K ops,
and now syncs are in the 5s neighborhood. Uh oh! Our syncs aren't amortizing
as well as we'd hope!

[malloc-exp/veri-8mb-wka-ioperf-disk-syncops-2.data]
------------------------------------------------------------------------------
Okay, let's just turn off sync and make the cache small (500MB) so we can
watch the perf cliff happen much sooner.
And then we'll probably need to be counting the cost of these crazy IOs,
maybe inside the system.

Modified workload-a to have sync at a zillion.
tools/run-veri-config-experiment.py workload=a device=disk ram=1.0gb config-8mb > malloc-exp/veri-1gb-8mb-wka-nosync-ioperf-disk.data

Why aren't the writes from the inital load sync showing up?

tools/run-veri-config-experiment.py workload=a device=disk ram=1.0gb rocks > malloc-exp/rocks-1gb-wka-nosync-ioperf-disk.data

Current thoughts.

(1) So maybe instead of comparing things vertically, horizontally makes more
sense.  Maybe rocks is fitting 2M items (1GB/(512+24)) in its cache, and
because of various overheads, we're only fitting some smaller number. We should
collect that number. By that metric, we cross:

          3Kop/s   2KOps/s  1KOp/s
  rocks   3400     4100     6000
  veri    2400     2800     4200

...So maybe we should throw some more RAM at veri and see how it does.
...And collect the actual storage size as viewed from inside.

(2) It's difficult to make a useful io analysis comparison between
the two systems, because rocks is measuring os read, not IO read. Can
we get real underlying IO numbers out of cgroups or /proc? Can we
use a system counter and just hope the rest of the system is fairly
quiescent? iostat and pidstat can capture this; they must look in /proc
/proc/self/io has read_bytes and write_bytes fields that are interesting.


tools/run-veri-config-experiment.py workload=a device=disk ram=1.0gb config-8mb > malloc-exp/veri-1gb-8mb-wka-nosync-ioperf-disk-2.data
(run manually after compile to avoid cgroup killage)
This run:
* sets malloc overhead to 0.78 to compensate for observed typical block
  loading of 6.25MiB
* re-enables infrequent aggregate collections
  (to count up used memory from inside Dafny, so we can simulate perfectly
  effecient memory usage)
* adds proc-io output to track "real" disk IO.

tools/run-veri-config-experiment.py workload=a device=disk ram=1.0gb rocks > malloc-exp/rocks-1gb-wka-nosync-ioperf-disk-2.data
* adds proc-io output to track "real" disk IO.

Looks like proc-io is going to be a different kind of lies for veri. I'm
seeing almost no real reads. I guess they're getting absorbed by the buffer
cache?

Oh hells yeah, the "horizontal offset hypothesis" seems to be supported;
we're blowing way past rocks now.

            3Kop/s   2KOps/s  1KOp/s
  rocks     3400     4100     6000
  veri      2400     2800     4200
  veri-1gb  4800     5600     ____
Hah. Look at that. We turned the memory usage up by not quite a factor of
two (1.87X), and the througput milestones all move out by a factor of two.
Wait, is that how that should work? Yeah, because we get twice as many
write ops in before we reach a given cache hit rate.

Huh, memory usage is falling way off; is that because we're not counting
PKVs? Yeah, probably. Yeah, exactly; they're "weirdBuckets" right now.

So here's what was happening. We never saw rocks hit rocks bottom
because we didn't wait long enough; it was able to stay in a warm-cache regime
much longer. Veri fell off the cliff sooner because it was utilizing much
less memory.

malloc-exp/veri-1gb-8mb-wka-nosync-ioperf-disk-3.data
accum counts pkvs.
...or, perhaps it doesn't. Sure doesn't seem to be.

Burning questions:
[ ] Are we counting memory accurately for the no-fragmentation simulation?
[ ] Is veri cheating by using buffer cache since I worked around cgroups?

------------------------------------------------------------------------------
ssh splinter1


tools/run-veri-config-experiment.py workload=a device=disk ram=1.0gb rocks > malloc-exp/rocks-1gb-wka-nosync-disk-1.data

sshfs -o allow_other,default_permissions -o reconnect -C -o ServerAliveInterval=15 -o workaround=all -o idmap=user jonh@splinter1:. splinter1

tools/run-veri-config-experiment.py workload=a device=disk ram=1.0gb config-8mb > malloc-exp/veri-1gb-100mb-8mb-wka-nosync-disk-1.data

[malloc-exp/veri-1gb-100mb-8mb-wka-nosync-disk-1.data-perf.png]
was an experiment where we used 100mb for veribetrfs internal cache,
and let the page cache supply everything else. What we learned was that
reading 8MB from the *page cache* was actually a bottleneck; we're using
75% system time (and 25% user, of which 20% was CRC32) to run memcpy at
3GBps. So we really can't get much out of the page cache without subblocks.

Upcoming strategies:
- repro the best case with conservative (MALLOC_OVERHEAD=2) cgroups.
  [malloc-exp/veri-1gb-512mb-8mb-wka-nosync-disk-1.data]
- try dropping in a better malloc (jemalloc) to help with fragmentation
  -- wah. It sucked. Still at 1.82X.
- see if PackedKVs can help with fragmentation
- try a dynamic oracle that watches the system page usage to try to
  stay tight on it.
- rob's compacting allocator

jemalloc looks pretty good on fragmentation; 1.1X:
http://ithare.com/testing-memory-allocators-ptmalloc2-tcmalloc-hoard-jemalloc-while-trying-to-simulate-real-world-loads/

tools/run-veri-config-experiment.py workload=a device=disk ram=1.0gb config-8mb > malloc-exp/veri-1gb-8mb-wka-nosync-ioperf-disk-jemalloc-1.data

jemalloc mysteries:
- how is it that we start out with 0.9GB allocated ("mem from inside veri"),
  never seeing the climb?
- why does jem go behind that?
  * Resolution: plot library was extrapolating Traces to the left of their
  first observation. We don't get the first accumulator data until 3Mops.
- why does jem see 18% more stuff than we think is on the heap?
  * Investigation: turn on malloc accounting again!
- is it possible that the gap from jem_mapped to os_mem is not billed to us?
- all reports of memory allocation climb to 1GB way sooner than we exhaust
  the veri cache

tools/plot/perf-compare.py malloc=malloc-exp/veri-1gb-8mb-wka-nosync-ioperf-disk-malloc-4.data jemalloc=malloc-exp/veri-1gb-8mb-wka-nosync-ioperf-disk-jemalloc-3.data

[ ] track master
[ ] check what cgroups measures, compare to jemalloc mapped
  https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt
  cat /sys/fs/cgroup/memory/VeribetrfsExp/memory.usage_in_bytes
[ ] study why jemalloc active seems higher than malloc
[ ] run under strace and see what's being given back under madvise

1.6 experiment seems to have fallen off into swapville

1. Capture swapping values from /sys/fs/cgroup/memory/VeribetrfsExp/memory.stat
2. Try using direct IO to keep cgroups from swapping us.
Yup.

[malloc-exp/veri-1gb-8mb-wka-nosync-ioperf-disk-jemalloc-7.data]
In this experiment, I told Veri to use 1gb, and told cgroups to limit us
to 1.8gb. Our actual usage before we fill the cache is around 1.4gb; notice
that "jem mapped" and "cgroups-usage" are similar in the Grand Unified Memory
graph.

Once we start paging in Veri blocks (around opn 3500), our jem_mapped usage
(orange line) sees a bump. The jem_alloc (green) line doesn't, so presumably
we are seeing a more modest version of the earlier problem: we changed the
size of cheese we usually allocate, so we're leaving a batch of holes behind
at the previous sizes.

But even that bump only goes to 1.6gb. Notice that the cgroups usage (red line)
bumps up to 1.8gb, and stays there. Presumably that difference is page cache
usage, now that we're actually faulting stuff in through the kernel IO path.
At the same time, look at the cgroups-stat-pgfault graph: we begin faulting
the application like crazy, eventually doing 150 pgfaults per opn! This
explains why the op throughput drops to 100io/sec.

But why are we pgfaulting? The application's 1.6gb fits in our budget. Is
the kernel deciding to fault our memory rather than evict our page cache
pages? Eww!

[malloc-exp/veri-1gb-8mb-wka-nosync-ioperf-disk-jemalloc-8.data]
...enables USE_DIRECT in Framework.cpp to try to bypass the page cache
altogether, so that hopefully cgroups notices that we don't actually
want its stupid help.
So with USE_DIRECT, we see interesting phenomena:
- cgroups-usage goes DOWN when we exceed the cache; in fact, it stays
near jem_alloc -- and in fact sometimes below! It seems we're getting
a cgroups discount for nonresident pages. That's pretty great!
- Yet we're still swapping!

[malloc-exp/veri-1gb-8mb-wka-nosync-ioperf-disk-jemalloc-9.data]
Rob suggests: run in a huge cgroup to avoid swapping; see how much
memory we really use. Setting cheat_factor to 10

[malloc-exp/veri-1gb-8mb-wka-nosync-ioperf-disk-jemalloc-10.data]
Same thing, no cgroup to see if faults go away

Okay, look, things only started taking this weird swappy turn after -3.

master merge: may need to advance dafny.
------------------------------------------------------------------------------
Did a huuuuge master merge. All of my explicit amassin' got lost in the
packedkv changeover.
Time to see if memory is still under control.

time build/VeribetrfsYcsb ycsb/workloada-onefield.spec /tmp/data/ --veribetrkv > malloc-exp/merge-1.data

Okay, back to the drawing board.
internal-accum-bytes is 1.23GiB for a 8MB*25600
* wait wut why is this system ever evicting? Debug symbols, I guess.
* malloc is much higher than internal-accum-bytes; check yer mallocs

merge-2: 200*8MB; internal-accum-bytes 1.21GiB but malloc 1.92GiB.
Time to break out the CountAmassAllocations call in YcsbMain.cpp.

merge-3: 40 * 8MB
internal-accum & underlying agree that we're using 0.27GiB, which suggests
we're not having amass troubles.
Malloc, on the other hand, sees 0.96GiB -- 3X more.
Most in explicit-seq.
  38 ~7.3MiB allocations -- those sound about right, actually.
...and 1M in 512-byte allocations -- values.
...and 3M in ~29 byte allocations -- keys.
Which is a lot, considering that (a) we already accounted for everything
and (b) I'd only expect 8MB/(24+512)*40 = ~626k kv pairs in cache.
Now the journal may be confounding things, except that the 100MB
JournalEntry array is already accounted for -- and there's precisely one
of it.
I suppose spend some time on the debug stack looking at the value allocations.

Journal: 1M entries, each 96B long! Each key, value pointer is 48B.
(32 bytes for DafnySequence + 16 bytes for dbg_underlying crap that
should be made switch-offable.)
So in fact we're probably ALSO accounting for the 1M value allocations
in the journal, and 1M of the 3M keys.

merge-4: Drop journal to 16Ki entries, which should cost around 10MB in
storage (vs 600MB). Yep, it does. Everything looks reasonably-well accounted
for; time to try a jemalloc experiment again.

merge-5: debug off, jemalloc on, tree size 200 * 8MB. Is the memory behavior
sane? (works around bug in os heap calculation) Yep, not too far off.
1.13GiB of internal becomes 1.6GiB of jem mapped; 41% overhead. Time to
try with cgroups.

merge-6: same config but with cgroups in play.
And run-veri-config is making journal-oblivious nonsense configs again.
8*1024*1024 * 7 
2 * 4096 + 2048 * 4096
          + 2 * 24*1024*1024

